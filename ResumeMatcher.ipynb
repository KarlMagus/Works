{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089c473f-dae2-4cd0-9a84-455c30c37965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Loading CSV data\n",
    "df = pd.read_csv(r'/users/karlm/downloads/candidates.csv')\n",
    "\n",
    "# Renaming columns to match the SQLite schema\n",
    "df.columns = ['Name', 'Contact_Details', 'Location', 'Job_Skills', 'Experience', 'Projects', 'Comments']\n",
    "\n",
    "# Connecting to SQLite database\n",
    "conn = sqlite3.connect('candidates.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# We drop the existing table if it exists`\n",
    "c.execute('DROP TABLE IF EXISTS Candidates')\n",
    "\n",
    "# Create a table with the updated schema\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS Candidates (\n",
    "             id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "             Name TEXT,\n",
    "             Contact_Details TEXT,\n",
    "             Location TEXT,\n",
    "             Job_Skills TEXT,\n",
    "             Experience TEXT,\n",
    "             Projects TEXT,\n",
    "             Comments TEXT\n",
    "             )''')\n",
    "\n",
    "# Insert our data into the table\n",
    "df.to_sql('Candidates', conn, if_exists='append', index=False)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99552395-948a-413d-983f-1ef27afdce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\karlm\\anaconda3\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from faiss-cpu) (23.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df5b22f-cfbe-4a40-821c-58e6a90010c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a56323-ac81-47b1-af09-6e694be1d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karlm\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\karlm\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Connect to SQLite database and fetch candidate data\n",
    "conn = sqlite3.connect('candidates.db')\n",
    "df = pd.read_sql_query(\"SELECT * FROM Candidates\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Combine all text fields to create a single text representation for each candidate\n",
    "df['combined_text'] = df[['Name', 'Contact_Details', 'Location', 'Job_Skills', 'Experience', 'Projects', 'Comments']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Generate embeddings for each candidate\n",
    "candidate_embeddings = model.encode(df['combined_text'].tolist())\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = candidate_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(candidate_embeddings)\n",
    "\n",
    "# Save the FAISS index and candidate IDs\n",
    "faiss.write_index(index, 'candidates.index')\n",
    "df[['id']].to_csv('candidate_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647c749f-20dc-4eb1-8aaa-33cc23546fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karlm\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching candidate IDs: [ 43  35 116  23 107]\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and FAISS index\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "index = faiss.read_index('candidates.index')\n",
    "candidate_ids = pd.read_csv('candidate_ids.csv')\n",
    "\n",
    "def search_candidates(job_description, top_k=5):\n",
    "    # Generate embedding for the job description\n",
    "    job_embedding = model.encode([job_description])\n",
    "\n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(job_embedding, top_k)\n",
    "\n",
    "    # Get the matching candidate IDs\n",
    "    matching_ids = candidate_ids.iloc[indices[0]]['id'].values\n",
    "\n",
    "    return matching_ids\n",
    "\n",
    "# Example usage\n",
    "job_description = \"Looking for a Machine Learning expert who knows Java.\"\n",
    "matching_ids = search_candidates(job_description)\n",
    "print(\"Matching candidate IDs:\", matching_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030643c7-bfef-46cc-9dfc-80f696ab4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\karlm\\anaconda3\\lib\\site-packages (4.40.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\karlm\\anaconda3\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karlm\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff0740d-5c08-4ad9-ba7f-1fbc0289765d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Resume_html</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
       "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
       "\n",
       "                                         Resume_html Category  \n",
       "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "4  <div class=\"fontsize fontface vmargins hmargin...       HR  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Alpha dataset\n",
    "alpha_df = pd.read_csv(r'/users/karlm/downloads/archive/Resume/Resume.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "alpha_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded926c3-5420-4f3c-a22e-5d8116b4e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karlm\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Load Alpha dataset\n",
    "alpha_df = pd.read_csv(r'/users/karlm/downloads/archive/Resume/Resume.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0481e-6a96-4f04-a37b-ae5fffc1e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings\n",
    "def get_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Get embeddings for all resumes\n",
    "embeddings = []\n",
    "for resume in alpha_df['Resume_str']:\n",
    "    embeddings.append(get_embeddings(resume, tokenizer, model))\n",
    "\n",
    "# Convert embeddings list to numpy array\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# Save embeddings to a file for later use\n",
    "np.save('resume_embeddings.npy', embeddings)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42cce52b-ff26-4c4f-84db-bb82a6c8a278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load('resume_embeddings.npy')\n",
    "\n",
    "# Create FAISS index\n",
    "d = embeddings.shape[1]  # dimension of embeddings\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the index to disk\n",
    "faiss.write_index(index, 'alpha_index.faiss')\n",
    "\n",
    "print(\"Index created and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b5b6af-953a-4fae-b1d3-9b48e0d58fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karlm\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 7:21:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.455700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load Alpha dataset\n",
    "alpha_df = pd.read_csv(r'/users/karlm/downloads/archive/resume/resume.csv')\n",
    "\n",
    "# Prepare data for training\n",
    "# For simplicity, let's assume we are doing binary classification on the 'Category' column\n",
    "alpha_df = alpha_df[['Resume_str', 'Category']]\n",
    "alpha_df['labels'] = alpha_df['Category'].factorize()[0]  # Convert categories to numerical labels\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    alpha_df['Resume_str'].tolist(), alpha_df['labels'].tolist(), test_size=0.2\n",
    ")\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(alpha_df['labels'].unique()))\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert to PyTorch dataset\n",
    "class ResumeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ResumeDataset(train_encodings, train_labels)\n",
    "val_dataset = ResumeDataset(val_encodings, val_labels)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine-tuned-distilbert')\n",
    "tokenizer.save_pretrained('./fine-tuned-distilbert')\n",
    "\n",
    "print(\"Fine-tuning completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc948123-fe6c-49f2-ad8d-261ebd4cf414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 20, 5, 0, 11]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = './fine-tuned-distilbert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, output_hidden_states=True)\n",
    "\n",
    "# Load the FAISS index and the associated embeddings\n",
    "index = faiss.read_index(\"alpha_index.faiss\")\n",
    "embeddings = np.load(\"resume_embeddings.npy\")\n",
    "alpha_df = pd.read_csv(r'/users/karlm/downloads/archive/Resume/Resume.csv')\n",
    "\n",
    "# Check the dimensions of the index\n",
    "assert embeddings.shape[1] == index.d, f\"Embeddings dimension {embeddings.shape[1]} does not match index dimension {index.d}\"\n",
    "\n",
    "# Function to get embeddings for a query using AutoModelForSequenceClassification\n",
    "def get_query_embedding(query, tokenizer, model):\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.hidden_states[-1]  # Get the hidden states from the last layer\n",
    "    return hidden_states.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Function to search FAISS index\n",
    "def search_faiss_index(query, k=5):\n",
    "    query_embedding = get_query_embedding(query, tokenizer, model)\n",
    "    assert query_embedding.shape[1] == index.d, f\"Query embedding dimension {query_embedding.shape[1]} does not match index dimension {index.d}\"\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = alpha_df.iloc[indices[0]]\n",
    "    return results\n",
    "\n",
    "# Function to predict categories using the fine-tuned model\n",
    "def predict_categories(resumes, tokenizer, model):\n",
    "    categories = []\n",
    "    for resume in resumes:\n",
    "        inputs = tokenizer(resume, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.detach().numpy()\n",
    "        predicted_label = np.argmax(logits, axis=1)[0]\n",
    "        categories.append(predicted_label)\n",
    "    return categories\n",
    "\n",
    "# Example usage\n",
    "query = \"I am looking for a data scientist position in healthcare\"\n",
    "relevant_resumes = search_faiss_index(query)\n",
    "predicted_categories = predict_categories(relevant_resumes['Resume_str'], tokenizer, model)\n",
    "print(predicted_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026cd34c-abe1-4326-859b-2e37d6b12b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit):  Looking for a data scientist position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant resumes:\n",
      "Resume ID: 34141299, Category: AGRICULTURE\n",
      "Resume Text:          TELEPHONE INTERVIEWER       Summary    Organized, task oriented professional with experience in customer support.  Background includes skilled active listener, strong customer service experience and ability to respond to the end-user in a business professional manner.  I am highly effective in using technical domain knowledge.  Self-starter committed to taking ownership and capable of completing assigned projects  independently and in a team environment.      Highlights          Data En...\n",
      "================================================================================\n",
      "Resume ID: 32042584, Category: BUSINESS-DEVELOPMENT\n",
      "Resume Text:          BUSINESS DEVELOPMENT INTERN       Summary    Obtain a position in analytics or data science in which I can enable data-driven decision-making to help leaders solve problems.      Highlights        Proficient using Statistical Analysis Software (SAS), R, SAS Data Miner, SQL, Relational Databases, and Microsoft Office programs. \n",
      "*Experienced in statistical analyses, sampling techniques, research design, C-level presentations, and professional writing skills.              Experience      B...\n",
      "================================================================================\n",
      "Resume ID: 25080805, Category: BANKING\n",
      "Resume Text:          INTERNSHIP           Professional Overview     experiences collecting and analyzing data with statistical methods, familiar with R and SAS programing, great knowledge of experiment design, sampling techniques and documents management. strong skills in communication, group-working and work-planning.        Core Qualifications          Strong knowledge of SAS, R and SSPS programming  Excellent research skills  Microsoft Word, Excel, PowerPoint  Excellent quantitative skills      Team lead...\n",
      "================================================================================\n",
      "Resume ID: 12011623, Category: ENGINEERING\n",
      "Resume Text:          ENGINEERING AND QUALITY TECHNICIAN       Career Overview    A highly experienced skilled graduate with Analytics degree with a very good experience in SAS, Web scraping, SQL, Predictive modelling and data visualization. Excellent ability in identifying data requirements for analysis, data cleaning, munging and model building; Ensures the organization uses it effectively to reach profit and growth objectives. Comfortable with data handling, modeling, and coding, and have an appreciation ...\n",
      "================================================================================\n",
      "Resume ID: 27375577, Category: BUSINESS-DEVELOPMENT\n",
      "Resume Text:          VP OF BUSINESS DEVELOPMENT       Professional Overview     Accomplished Business Development executive with 7 years of experience in life science startups and clinical research organizations. I also have an entrepreneurial spirit as the co-founder of data analytics company, and a financial background through my Master's at Bentley University.        Education     December 2012       Finance    McCallum Graduate School at Bentley University          Finance Extensive case-based exercises...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit):  Looking for a Business Development Intern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant resumes:\n",
      "Resume ID: 81508860, Category: DIGITAL-MEDIA\n",
      "Resume Text:          DIGITAL MARKETING DIRECTOR           Summary    With a background in Marketing and Event production, I extend my passion for exploiting new ways to enhance communication and networking actions to manage business processes more effectively, and engaging in active dialogues with global clients. International Marketing development and Event production is my preference for hiring. While working in a multi cultural environments, I believe diversity has a great impact on a company's growth an...\n",
      "================================================================================\n",
      "Resume ID: 27375577, Category: BUSINESS-DEVELOPMENT\n",
      "Resume Text:          VP OF BUSINESS DEVELOPMENT       Professional Overview     Accomplished Business Development executive with 7 years of experience in life science startups and clinical research organizations. I also have an entrepreneurial spirit as the co-founder of data analytics company, and a financial background through my Master's at Bentley University.        Education     December 2012       Finance    McCallum Graduate School at Bentley University          Finance Extensive case-based exercises...\n",
      "================================================================================\n",
      "Resume ID: 26278597, Category: BUSINESS-DEVELOPMENT\n",
      "Resume Text:          BUSINESS DEVELOPMENT MANAGER       Summary     An experienced manager who is highly motivated and has vast knowledge of the retail industry. As a perfect role model for the team, able to coach, give feedback, build morale, roll out initiatives, and make recommendations on merchandising and product presentation. Able to handle high-profile and hands-on management roles that require commercial acumen and creative flair. As an exceptional person and  also able to drive brand availability, ...\n",
      "================================================================================\n",
      "Resume ID: 21363048, Category: CONSTRUCTION\n",
      "Resume Text:          DIRECTOR OF FACILITIES AND CONSTRUCTION       Executive Profile    As a Mechanical Engineer, I find it tremendously rewarding to problem-solve and build a legacy for high-end residential construction and commercial endeavors. It's exciting to work with other business owners who have a great vision and want to build a structure around those dreams. This is my business plan! With honesty, creativity, good teams and hard work, we can help put a structure to your imagination. With over 15 y...\n",
      "================================================================================\n",
      "Resume ID: 24574164, Category: DIGITAL-MEDIA\n",
      "Resume Text:          SENIOR DIRECTOR, PRODUCT MANAGEMENT       Career Overview    For twenty years, I've done product management, product marketing, and business development in organizations from early stage start-up to large, publicly traded companies. Common in every role at every company is finding success through a laser-like focus on the business needs of customers. With over a decade of online media experience, I remain fascinated by the intersection of media creation, distribution, consumption, and m...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit):  Pick up the top 10 profiles for the following job description, We are looking for a skilled UI Developer to join our dynamic team. The ideal candidate will have a strong background in front-end development, with proficiency in HTML, CSS, JavaScript, and modern frameworks like React or Angular. Your primary responsibility will be to create visually appealing and user-friendly web interfaces that enhance user experience and align with our brand guidelines.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant resumes:\n",
      "Resume ID: 80679862, Category: DIGITAL-MEDIA\n",
      "Resume Text:          DIGITAL MARKETING SPECIALIST             Highlights          Strong Digital Marketing experience using Social Media platforms  Proficient in the following programs, tools, and applications:  Slack, Google Analytics, AdWords, Site Catalyst / Omniture / Adobe Analytics, Microsoft Excel and Word / Apple Pages and Numbers, Adobe Creative (Photoshop, Lightroom, Illustrator) Final Cut Pro, Avid, Mail Chimp, Constant Contact, Work Zone, Mind Jet, Salesforce, GitLab, GitHub, CVS, WordPress, Dre...\n",
      "================================================================================\n",
      "Resume ID: 21297521, Category: BANKING\n",
      "Resume Text:          MANAGER           Experience      Manager  ,   11/2013   to   Current     Company Name   –   City  ,   State      Manage a large global team of up to 50 Managers, Technical Leads and Software Developers across a portfolio of 25 LOB applications.  My responsibilities include:.  Vendor management, RFPs, vendor selection, scope based contractual obligations, and negotiations on Fixed Bid, Fixed Scope, T&M SOWs and 3rd Party Software licensing.  LRPs, short and long-term strategic Road Maps...\n",
      "================================================================================\n",
      "Resume ID: 18354623, Category: DIGITAL-MEDIA\n",
      "Resume Text:          DIGITAL MARKETING MANAGER       Career Focus    Digital Marketing Manager Accomplished professional in digital marketing, digital project management, content management and migration, SEO, social media and web analytics. Identify, manage and implement web based solutions for a variety of online initiatives including multi-national/multi-lingual website development. Act as the technical lead in digital marketing decisions with the keen ability to keep projects moving forward in the face ...\n",
      "================================================================================\n",
      "Resume ID: 85421438, Category: HEALTHCARE\n",
      "Resume Text:          PA MEDIA GROUP       Summary    Be in a position involving and utilizing my marketing and management skills and knowledge gained throughout my education and on the job experience.      Highlights        Deep understanding of Google Analytics; analyzing website traffic and trends to help make business decisions; experienced in the tracking and optimize advertising campaigns; Heavy experience in digital marketing (search, target ads, email, social, display, mobile); sold and managed numer...\n",
      "================================================================================\n",
      "Resume ID: 24574164, Category: DIGITAL-MEDIA\n",
      "Resume Text:          SENIOR DIRECTOR, PRODUCT MANAGEMENT       Career Overview    For twenty years, I've done product management, product marketing, and business development in organizations from early stage start-up to large, publicly traded companies. Common in every role at every company is finding success through a laser-like focus on the business needs of customers. With over a decade of online media experience, I remain fascinated by the intersection of media creation, distribution, consumption, and m...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = './fine-tuned-distilbert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load the FAISS index and the associated embeddings\n",
    "index = faiss.read_index(\"alpha_index.faiss\")\n",
    "embeddings = np.load(\"resume_embeddings.npy\")\n",
    "alpha_df = pd.read_csv(r'/users/karlm/downloads/archive/Resume/Resume.csv')\n",
    "\n",
    "# Function to get embeddings for a query\n",
    "def get_query_embedding(query, tokenizer, model):\n",
    "    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model.base_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Function to search FAISS index\n",
    "def search_faiss_index(query, k=5):\n",
    "    query_embedding = get_query_embedding(query, tokenizer, model)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = alpha_df.iloc[indices[0]]\n",
    "    return results\n",
    "\n",
    "# Command-line interface for querying\n",
    "def command_line_interface():\n",
    "    while True:\n",
    "        query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        relevant_resumes = search_faiss_index(query)\n",
    "        print(\"Relevant resumes:\")\n",
    "        for idx, row in relevant_resumes.iterrows():\n",
    "            print(f\"Resume ID: {row['ID']}, Category: {row['Category']}\")\n",
    "            print(f\"Resume Text: {row['Resume_str'][:500]}...\")  # Print the first 500 characters of the resume\n",
    "            print(\"=\"*80)\n",
    "\n",
    "# Run the command-line interface\n",
    "command_line_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad275ca-89d3-42c7-83dc-20a7b00c7d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
