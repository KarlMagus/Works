{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxGTEXdR1ZF-",
        "outputId": "472ebe94-bfbc-4217-d921-810f0a5992f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ea97b5324680>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_image_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'np_utils' from 'keras.utils' (/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Import Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "# Import tensorflow as the backend for Keras\n",
        "from keras import backend as k\n",
        "k.set_image_data_format('channels_first')\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "# Import required libraries for cnfusion matrix\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "PATH = os.getcwd()\n",
        "# Define data path\n",
        "data_path = '/content/gdrive/MyDrive/archive/data/data'\n",
        "data_dir_list = os.listdir(data_path)\n",
        "data_dir_list\n",
        "\n",
        "#Print the first 5 rows of the dataframe.\n",
        "#diabetes_data.head()"
      ],
      "metadata": {
        "id": "3wYi1Exa66zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows=128\n",
        "img_cols=128\n",
        "num_channel=1\n",
        "num_epoch=100\n",
        "# Define the number of classes\n",
        "num_classes = 7\n",
        "img_data_list=[]\n",
        "for dataset in data_dir_list:\n",
        "\timg_list=os.listdir(data_path+'/'+ dataset)\n",
        "\tprint ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
        "\tfor img in img_list:\n",
        "\t\tinput_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
        "\t\tinput_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
        "\t\tinput_img_resize=cv2.resize(input_img,(128,128))\n",
        "\t\timg_data_list.append(input_img_resize)\n",
        "\n",
        "img_data = np.array(img_data_list)\n",
        "img_data = img_data.astype('float32')\n",
        "img_data /= 255\n",
        "print (img_data.shape)"
      ],
      "metadata": {
        "id": "8F7F5zK-XkAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if num_channel==1:\n",
        "\tif k.image_data_format()=='th':\n",
        "\t\timg_data= np.expand_dims(img_data, axis=1)\n",
        "\t\tprint (img_data.shape)\n",
        "\telse:\n",
        "\t\timg_data= np.expand_dims(img_data, axis=3)\n",
        "\t\tprint (img_data.shape)\n",
        "\n",
        "else:\n",
        "\tif K.image_dim_ordering()=='th':\n",
        "\t\timg_data=np.rollaxis(img_data,3,1)\n",
        "\t\tprint (img_data.shape)"
      ],
      "metadata": {
        "id": "yEPWSlc0X3cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_classes = 7\n",
        "num_of_samples = img_data.shape[0]\n",
        "labels = np.ones((num_of_samples,),dtype='int64')\n",
        "labels[0:365]=0\n",
        "labels[365:567]=1\n",
        "labels[567:987]=2\n",
        "labels[987:1189]=3\n",
        "labels[1189:1399]=4\n",
        "labels[1399:1601]=5\n",
        "labels[1601:1803]=6\n",
        "names = ['bike', 'cars', 'cats', 'dogs', 'flowers', 'horses', 'human']\n"
      ],
      "metadata": {
        "id": "ErLtD1tTGvsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np_utils.to_categorical(labels, num_classes)"
      ],
      "metadata": {
        "id": "WyQBhTPdG0wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x,y = shuffle(img_data,Y, random_state=2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)"
      ],
      "metadata": {
        "id": "NdrNMwB-G1ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape = {}\".format(X_train.shape))\n",
        "print(\"X_test shape = {}\".format(X_test.shape))"
      ],
      "metadata": {
        "id": "iKzMOhqHG9ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train= np.squeeze(X_train)\n",
        "#X_test = np.squeeze(X_test)\n",
        "#print(\"X_train shape = {}\".format(X_train.shape))\n",
        "#print(\"X_test shape = {}\".format(X_test.shape))"
      ],
      "metadata": {
        "id": "PhBJQK7pX8T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image = X_train[1203,:].reshape((128,128))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sYTQ5qc5HBTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=img_data[0].shape\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "tT2_51iVnXZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Re-writing the kaggle cnn code insteps because that was showing shape ncompatibe\n",
        "# from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# from keras.models import Model\n",
        "# #Initialising the input shape\n",
        "# input_shape=(128, 128, 1)\n",
        "# #with input shape 128,128,32 it shows fitting issues and the summary of the kaggle notebook differs a lot too.\n",
        "# #With input shape 128,128,32 the output parameters summary of this and the kaggle notebook look very similar.\n",
        "\n",
        "# # Define the input layer\n",
        "# inputs = Input(shape=input_shape)\n",
        "\n",
        "# # Add the convolutional layers\n",
        "# conv1 = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
        "# conv2 = Conv2D(32, (3, 3), padding='same', activation='relu')(conv1)\n",
        "# # Add the pooling layer\n",
        "\n",
        "# pooling1 = MaxPooling2D(pool_size=(2, 2), strides=(2,2),padding='same',data_format='channels_last')(conv2)\n",
        "# #I tried adding padding and strides in this pool layer too shows error: Default MaxPoolingOp only supports NHWC on device type CPU\n",
        "\n",
        "# # Add the dropout layer\n",
        "# dropout1 = Dropout(0.5)(pooling1)\n",
        "\n",
        "# # Add the flatten layer\n",
        "# flatten = Flatten()(dropout1)\n",
        "\n",
        "# # Add the fully connected layers\n",
        "# dense1 = Dense(128, activation='relu')(flatten)\n",
        "# dropout2 = Dropout(0.5)(dense1)\n",
        "# outputs = Dense(num_classes, activation='softmax')(dropout2)\n",
        "\n",
        "# # Define the model\n",
        "# cnn_model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "HI9jfRJUHB7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn_model.compile(loss='categorical_crossentropy', optimizer='adadelta',metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "UWKjuEpFIYPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn_model.summary()"
      ],
      "metadata": {
        "id": "AQ5IsRBHLI_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y_train shape = {}\".format(y_train.shape))\n",
        "print(\"y_test shape = {}\".format(y_test.shape))\n",
        "print(\"X_test shape = {}\".format(X_test.shape))"
      ],
      "metadata": {
        "id": "TLajBwS8dLO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting\n",
        "#history = cnn_model.fit(X_train, y_train, epochs=num_epoch, batch_size=16, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "TFEn-8rt_zgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#tried another cnn model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "# Define the CNN model\n",
        "model = models.Sequential([\n",
        "    # Convolutional layers\n",
        "    layers.Conv2D(32, (3, 3),padding='same', activation='relu', input_shape=(128, 128, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), padding='same',activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3),padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2), padding='same', strides=(1,1)),\n",
        "\n",
        "    # Flatten output from convolutional layers\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Dense layers\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])'''\n"
      ],
      "metadata": {
        "id": "y9iyducpBdk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rewriting the above code insteps:\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.models import Model\n",
        "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "  tf.config.experimental.set_memory_growth(gpu, True)\n",
        "# Define the input shape\n",
        "input_shape = (128,128,32)\n",
        "\n",
        "# Define the input layer\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "'''# Add the convolutional layer\n",
        "conv1 = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
        "#adding the pooling layer\n",
        "pooling1= MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "# Add the convolutional layer\n",
        "conv2 = Conv2D(32, (3, 3), padding='same', activation='relu')(pooling1)\n",
        "\n",
        "# Add the pooling layer\n",
        "pooling2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "# Add the convolutional layer\n",
        "conv3 = Conv2D(128, (3, 3), padding='same', activation='relu')(pooling2)\n",
        "\n",
        "# Add the pooling layer\n",
        "pooling3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "# Add the dropout layer\n",
        "dropout1 = Dropout(0.5)(pooling2)\n",
        "\n",
        "# Add the flatten layer\n",
        "flatten = Flatten()(dropout1)\n",
        "\n",
        "# Add the fully connected layers\n",
        "dense1 = Dense(128, activation='relu')(flatten)\n",
        "dropout2 = Dropout(0.5)(dense1)\n",
        "outputs = Dense(num_classes, activation='softmax')(dropout2)\n",
        "\n",
        "# Define the model\n",
        "cnn_model = Model(inputs=inputs, outputs=outputs)'''\n",
        "\n"
      ],
      "metadata": {
        "id": "7kN8UBS_r0JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add the convolutional layer\n",
        "conv1 = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
        "\n",
        "# Add the convolutional layer\n",
        "conv2 = Conv2D(32, (3, 3), padding='same', activation='relu')(conv1)\n",
        "\n",
        "# Add the convolutional layer\n",
        "conv3 = Conv2D(128, (3, 3), padding='same', activation='relu')(conv2)\n",
        "\n",
        "# Add the dropout layer\n",
        "dropout1 = Dropout(0.5)(conv3)\n",
        "\n",
        "# Add a larger flatten layer to match the expected input shape\n",
        "flatten = Flatten()(dropout1)\n",
        "\n",
        "# Add the fully connected layers\n",
        "dense1 = Dense(524288, activation='relu')(flatten)\n",
        "dropout2 = Dropout(0.5)(dense1)\n",
        "outputs = Dense(num_classes, activation='softmax')(dropout2)\n",
        "\n",
        "# Define the model\n",
        "cnn_model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "RTRJG_SoGBJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "jVml1nvrDNpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DLS1WdaKshxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "cnn_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "_dIFysVlDURM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss=hist.history['loss']\n",
        "val_loss=hist.history['val_loss']\n",
        "train_acc=hist.history['acc']\n",
        "val_acc=hist.history['val_acc']\n",
        "xc=range(num_epoch)"
      ],
      "metadata": {
        "id": "Q7ZRLszh2WVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1,figsize=(10,5))\n",
        "plt.plot(xc,train_loss)\n",
        "plt.plot(xc,val_loss)\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train Loss vs Validation Loss')\n",
        "plt.grid(True)\n",
        "plt.legend(['Train Loss','Validation Loss'])\n",
        "plt.style.use(['classic'])"
      ],
      "metadata": {
        "id": "MJUgzDBi2ZUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}